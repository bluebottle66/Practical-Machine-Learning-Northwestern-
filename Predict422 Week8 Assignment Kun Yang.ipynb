{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:Download Embeddings data #\n",
    "\n",
    "- Install Python chakin package\n",
    "- obtain GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunya\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import chakin  \n",
    "\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling 50d twitter pretrained word vectors below using the code below ###\n",
    "\n",
    "### note that I also ran the jump-start code and downloaded the GloVe.6B.50d data already ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAKIN_INDEX = 18\n",
    "NUMBER_OF_DIMENSIONS = 50\n",
    "SUBFOLDER_NAME = \"glove.twitter.27B\"\n",
    "\n",
    "#CHAKIN_INDEX = 11\n",
    "#NUMBER_OF_DIMENSIONS = 50\n",
    "#SUBFOLDER_NAME = \"gloVe.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'embeddings'ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(\n",
    "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
    "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0:\n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "\n",
    "print('\\nRun complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import os.path  # for manipulation of file path names\n",
    "\n",
    "import re  # regular expressions\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "REMOVE_STOPWORDS = False  # no stopword removal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define two sizes of vocabulary below to our test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVOCABSIZE2 = 20000  # specify desired size of pre-defined embedding vocabulary\n",
    "#define two different vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_directory = 'C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/embeddings/gloVe.6B/'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_directory2 = 'C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/embeddings/glove.twitter.27B/'\n",
    "filename2 = 'glove.twitter.27B.50d.txt'\n",
    "embeddings_filename2 = os.path.join(embeddings_directory2, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now prepare the embeddings...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/embeddings/gloVe.6B/glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n",
      "\n",
      "Loading embeddings from C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/embeddings/glove.twitter.27B/glove.twitter.27B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")\n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename2)\n",
    "word_to_index2, index_to_embedding2 = \\\n",
    "    load_embedding_from_disks(embeddings_filename2, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the size of imported embeddings ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using a non-existing word to get the last index of the word_to_index for both embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n",
      "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
     ]
    }
   ],
   "source": [
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index2[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size2 = idx \n",
    "\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "\n",
    "\n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (1193515, 50)\n"
     ]
    }
   ],
   "source": [
    "#now test the second embeddings\n",
    "vocab_size2, embedding_dim2 = index_to_embedding2.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    the --> 0 --> [0.78704, 0.72151, 0.29148, -0.056527, 0.31683, 0.47172, 0.023461, 0.69568, 0.20782, 0.60985, -0.22386, 0.7481, -2.6208, 0.20117, -0.48104, 0.12897, 0.035239, -0.24486, -0.36088, 0.026686, 0.28978, -0.10698, -0.34621, 0.021053, 0.54514, -1.0958, -0.274, 0.2233, 1.0827, -0.029018, -0.84029, 0.58619, -0.36511, 0.34016, 0.89615, 0.32757, 0.24267, 0.68404, -0.34374, 0.13583, -2.2162, -0.42537, 0.46157, 0.88626, -0.22014, 0.025599, -0.38615, 0.080107, -0.075323, -0.61461]\n"
     ]
    }
   ],
   "source": [
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding2[idx])  # \"int\" for compact print only.\n",
    "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using a test sentence to check how embedding works**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now prepare the vocabulary, one with 10000, one with 20000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare data for the 2*2 experiments design #\n",
    "\n",
    "## prepare the vacabularies, one with 10000, one with 20000 ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "def default_factory2():\n",
    "    return EVOCABSIZE2  # last/unknown-word row in limited_index_to_embedding2\n",
    "\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "limited_word_to_index2 = defaultdict(default_factory2, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE2})\n",
    "\n",
    "limited_word_to_index3 = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index2.items() if v < EVOCABSIZE})\n",
    "\n",
    "limited_word_to_index4 = defaultdict(default_factory2, \\\n",
    "    {k: v for k, v in word_to_index2.items() if v < EVOCABSIZE2})\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "limited_index_to_embedding2 = index_to_embedding[0:EVOCABSIZE2,:]\n",
    "limited_index_to_embedding3 = index_to_embedding2[0:EVOCABSIZE,:]\n",
    "limited_index_to_embedding4 = index_to_embedding2[0:EVOCABSIZE2,:]\n",
    "\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "limited_index_to_embedding2 = np.append(limited_index_to_embedding2, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "limited_index_to_embedding3 = np.append(limited_index_to_embedding3, \n",
    "    index_to_embedding2[index_to_embedding2.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim2), \n",
    "    axis = 0)\n",
    "\n",
    "limited_index_to_embedding4 = np.append(limited_index_to_embedding4, \n",
    "    index_to_embedding2[index_to_embedding2.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim2), \n",
    "    axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check what we get above...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 50)\n",
      "(20001, 50)\n",
      "(10001, 50)\n",
      "(20001, 50)\n"
     ]
    }
   ],
   "source": [
    "print(limited_index_to_embedding.shape)\n",
    "print(limited_index_to_embedding2.shape)\n",
    "print(limited_index_to_embedding3.shape)\n",
    "print(limited_index_to_embedding4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean the momory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del index_to_embedding\n",
    "del index_to_embedding2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pulling in Movie Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not remove stopwords in this exercise because they are\n",
    "# important to keeping sentences intact\n",
    "if REMOVE_STOPWORDS:\n",
    "    print(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# previous analysis of a list of top terms showed a number of words, along \n",
    "# with contractions and other word strings to drop from further analysis, add\n",
    "# these to the usual English stopwords to be dropped from a document collection\n",
    "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "        've', 're', 'vs'] \n",
    "\n",
    "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "        'toni','welles','william','wolheim','nikita']\n",
    "\n",
    "    # start with the initial list and add to it for movie text work \n",
    "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "        some_proper_nouns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gathering Positive and Native movie review data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    #print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    #print('Sample string (Document %d) %s'%(i,words[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'C:/Users/kunya/OneDrive/Documents/NU_DL422/week8/movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data converting for the positive and negative review data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert the loaded positive and negative review data into embedding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings2 = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding2[limited_word_to_index2[word]]) \n",
    "    embeddings2.append(embedding)\n",
    "    \n",
    "embeddings3 = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding3[limited_word_to_index3[word]]) \n",
    "    embeddings3.append(embedding)\n",
    "    \n",
    "\n",
    "embeddings4 = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding4[limited_word_to_index4[word]]) \n",
    "    embeddings4.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the new embedding data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: story\n",
      "Embedding for this word:\n",
      " [ 3.4878e-01  1.1394e-01  2.8916e-02  2.1578e-01  5.3238e-01 -1.8473e-01\n",
      "  1.2152e+00  1.9563e-01 -1.2163e-01 -1.7920e-02  3.6078e-01 -3.1153e-01\n",
      " -3.9005e+00  2.8846e-01 -1.9596e-01  3.2511e-01  4.5758e-01  6.2017e-01\n",
      " -1.9086e-01  5.0725e-01 -3.9320e-02  6.1784e-02 -2.7220e-01  2.2608e-01\n",
      " -1.8481e-01 -3.3035e-01 -8.0972e-01 -3.2739e-01  6.7105e-02  2.6769e-01\n",
      " -3.2417e-01 -7.7919e-02 -6.2642e-01 -4.7699e-02  5.3610e-02  2.0151e-01\n",
      "  5.2316e-01 -6.3660e-01  6.9955e-01 -1.2729e+00 -9.4083e-01 -4.0108e-01\n",
      "  6.2220e-01  2.8639e-01  1.5763e-01  4.7197e-01  5.7900e-04 -6.0632e-02\n",
      " -2.6331e-01 -3.3637e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 3.4878e-01  1.1394e-01  2.8916e-02  2.1578e-01  5.3238e-01 -1.8473e-01\n",
      "  1.2152e+00  1.9563e-01 -1.2163e-01 -1.7920e-02  3.6078e-01 -3.1153e-01\n",
      " -3.9005e+00  2.8846e-01 -1.9596e-01  3.2511e-01  4.5758e-01  6.2017e-01\n",
      " -1.9086e-01  5.0725e-01 -3.9320e-02  6.1784e-02 -2.7220e-01  2.2608e-01\n",
      " -1.8481e-01 -3.3035e-01 -8.0972e-01 -3.2739e-01  6.7105e-02  2.6769e-01\n",
      " -3.2417e-01 -7.7919e-02 -6.2642e-01 -4.7699e-02  5.3610e-02  2.0151e-01\n",
      "  5.2316e-01 -6.3660e-01  6.9955e-01 -1.2729e+00 -9.4083e-01 -4.0108e-01\n",
      "  6.2220e-01  2.8639e-01  1.5763e-01  4.7197e-01  5.7900e-04 -6.0632e-02\n",
      " -2.6331e-01 -3.3637e-01]\n",
      "First word in first document: but\n",
      "Embedding for this word:\n",
      " [ 0.35934   -0.2657    -0.046477  -0.2496     0.54676    0.25924\n",
      " -0.64458    0.1736    -0.53056    0.13942    0.062324   0.18459\n",
      " -0.75495   -0.19569    0.70799    0.44759    0.27031   -0.32885\n",
      " -0.38891   -0.61606   -0.484      0.41703    0.34794   -0.19706\n",
      "  0.40734   -2.1488    -0.24284    0.33809    0.43993   -0.21616\n",
      "  3.7635     0.19002   -0.12503   -0.38228    0.12944   -0.18272\n",
      "  0.076803   0.51579    0.0072516 -0.29192   -0.27523    0.40593\n",
      " -0.040394   0.28353   -0.024724   0.10563   -0.32879    0.10673\n",
      " -0.11503    0.074678 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.35934   -0.2657    -0.046477  -0.2496     0.54676    0.25924\n",
      " -0.64458    0.1736    -0.53056    0.13942    0.062324   0.18459\n",
      " -0.75495   -0.19569    0.70799    0.44759    0.27031   -0.32885\n",
      " -0.38891   -0.61606   -0.484      0.41703    0.34794   -0.19706\n",
      "  0.40734   -2.1488    -0.24284    0.33809    0.43993   -0.21616\n",
      "  3.7635     0.19002   -0.12503   -0.38228    0.12944   -0.18272\n",
      "  0.076803   0.51579    0.0072516 -0.29192   -0.27523    0.40593\n",
      " -0.040394   0.28353   -0.024724   0.10563   -0.32879    0.10673\n",
      " -0.11503    0.074678 ]\n",
      "First word in first document: from\n",
      "Embedding for this word:\n",
      " [-3.8401e-01  1.0875e-01  2.8662e-01 -6.6156e-01  3.8833e-02 -3.0575e-01\n",
      "  3.6360e-01  8.2926e-02 -4.1019e-01 -9.5378e-01  4.2104e-01 -2.6638e-01\n",
      " -5.8528e+00 -1.3227e-01 -2.8717e-01 -4.3347e-01  3.5693e-01 -4.9739e-01\n",
      " -1.2091e-01 -1.3876e-02 -2.2298e-01 -7.2154e-01 -1.8397e-01 -5.3441e-01\n",
      " -4.2049e-02  4.3647e-01  1.8059e-01  8.7492e-02  8.3075e-01  2.0319e-01\n",
      " -2.1760e-01 -3.5566e-01 -5.2560e-01  5.1073e-02  7.1591e-01 -2.0299e-01\n",
      " -1.0518e-02  7.0557e-01  3.6345e-03 -3.2321e-01 -9.2623e-01 -3.7608e-01\n",
      " -1.3119e-01  1.0151e-01  1.4738e-01  1.8218e-01 -2.6578e-01 -5.5857e-02\n",
      "  1.5330e-01  2.4556e-01]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-3.8401e-01  1.0875e-01  2.8662e-01 -6.6156e-01  3.8833e-02 -3.0575e-01\n",
      "  3.6360e-01  8.2926e-02 -4.1019e-01 -9.5378e-01  4.2104e-01 -2.6638e-01\n",
      " -5.8528e+00 -1.3227e-01 -2.8717e-01 -4.3347e-01  3.5693e-01 -4.9739e-01\n",
      " -1.2091e-01 -1.3876e-02 -2.2298e-01 -7.2154e-01 -1.8397e-01 -5.3441e-01\n",
      " -4.2049e-02  4.3647e-01  1.8059e-01  8.7492e-02  8.3075e-01  2.0319e-01\n",
      " -2.1760e-01 -3.5566e-01 -5.2560e-01  5.1073e-02  7.1591e-01 -2.0299e-01\n",
      " -1.0518e-02  7.0557e-01  3.6345e-03 -3.2321e-01 -9.2623e-01 -3.7608e-01\n",
      " -1.3119e-01  1.0151e-01  1.4738e-01  1.8218e-01 -2.6578e-01 -5.5857e-02\n",
      "  1.5330e-01  2.4556e-01]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "#.........check for all the 4 embeddings\n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding3[limited_word_to_index3[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings3[0][0][:])\n",
    "\n",
    "# Show the seventh word in the tenth document\n",
    "test_word = documents[6][9]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding2[limited_word_to_index2[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings2[6][9][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', \n",
    "      limited_index_to_embedding4[limited_word_to_index4[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
    "      embeddings4[999][39][:])        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Build up the RNN models #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n",
    "embeddings_array2 = np.array(embeddings2)\n",
    "embeddings_array3 = np.array(embeddings3)\n",
    "embeddings_array4 = np.array(embeddings4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try to using the first set of data to test RNN model.... (10000 vocabulary from 6B.50d vector data) ###\n",
    "\n",
    "> Start with first experiments\n",
    "\n",
    "> split the training/testing set with 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-36-6985be14b3e1>:1: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Start training and evaluate the models on predictin accuracy #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.48 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.505\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.51 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Repeat the training routine for 3 other settings below #\n",
    "\n",
    "## The 2*2 experiments are combination of Embeddings Vectors (Wikipedia+Gigaword 5 vs Twitter); two sizes of vocabulary file (10000 vs 20000) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.505\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.54 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.54 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.57\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.66 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.665\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array2, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array2.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array2.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.54 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array3, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array3.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array3.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.66 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.66 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.66 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.705\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.685\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array4, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array4.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array4.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Conduct additional testing scenarios #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing multi-layer below ##\n",
    "\n",
    "> Additional experiments will compare to above Case 4 with same vector files/voculbulary/npochs/batch_size\n",
    "\n",
    "> below change from 2 layer to 3 layer deep RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.54\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.505\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.525\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.535\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array4, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array4.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array4.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "n_neurons = 100  # analyst specified number of neurons\n",
    "n_layers = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                      activation=tf.nn.relu)\n",
    "          for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "states_concat = tf.concat(axis=1, values=states)\n",
    "logits = tf.layers.dense(states_concat, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** test experiment 6, using the same setting as experiment 5, with n_epochs=100 and batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.52 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.39 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.52\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.57\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.58\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.66 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.71 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.6 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.715\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.71\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.705\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.72\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.74\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(embeddings_array4, thumbs_down_up, test_size=0.20, \n",
    "                     random_state = RANDOM_SEED)\n",
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array4.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array4.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "n_neurons = 100  # analyst specified number of neurons\n",
    "n_layers = 3\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "cells = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "         for layer in range(n_layers)]\n",
    "cells_drop = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)\n",
    "              for cell in cells]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells_drop)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n",
    "\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])\n",
    "\n",
    "states_concat = tf.concat(axis=1, values=states)\n",
    "logits = tf.layers.dense(states_concat, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "train_keep_prob = 0.5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch,keep_prob: train_keep_prob})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch,keep_prob: train_keep_prob})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test,keep_prob: train_keep_prob})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary #\n",
    "\n",
    "## I conducted a 2 * 2 experiments, plus two additional experiments with one more layer ##\n",
    "### Results shows using Twitter 50B Vector and 20000 Voculbulary have the better results, which will be my recommendation for management ###\n",
    "### As for the deep CNN test with one more layer, I see the train accuracy improve to 100% now, however, the test accuracy was not really improves comparing to case 4 ###\n",
    "### For last experiement, I appied Dropout into the model with train_keep_prob=0.5, looks like test accuracy get improved ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
